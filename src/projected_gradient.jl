
function projected_gradient(
    nlp,
    uk;
    max_iter=1000,
    ls_itermax=30,
    Œ±0=1.0,
    Œ≤=0.4,
    œÑ=1e-4,
    tol=1e-5,
    bfgs=false,
    verbose_it=Inf,
)

    u_prev = copy(uk)
    grad = copy(uk)
    wk = copy(uk)
    u‚ô≠ = nlp.inner.u_min
    u‚ôØ = nlp.inner.u_max
    H = I
    norm_grad = Inf
    n_iter = 0

    for i in 1:max_iter
        n_iter += 1
        # solve power flow and compute gradients
        ExaPF.update!(nlp, uk)

        # evaluate cost
        c = ExaPF.objective(nlp, uk)
        # Evaluate cost of problem without penalties
        c_ref = nlp.scaler.scale_obj * ExaPF.objective(nlp.inner, uk)
        ExaPF.gradient!(nlp, grad, uk)

        # compute control step
        # Armijo line-search (Bertsekas, 1976)
        dk = H * grad
        step = Œ±0
        for j_ls in 1:ls_itermax
            step *= Œ≤
            ExaPF.project!(wk, uk .- step .* dk, u‚ô≠, u‚ôØ)
            ExaPF.update!(nlp, wk)
            ft = ExaPF.objective(nlp, wk)
            if ft <= c - œÑ * dot(dk, wk .- uk)
                break
            end
        end

        # step = Œ±i
        wk .= uk .- step * dk
        ExaPF.project!(uk, wk, u‚ô≠, u‚ôØ)

        # Stopping criteration: u‚Çñ‚Çä‚ÇÅ - u‚Çñ
        ## Dual infeasibility
        norm_grad = norm(uk .- u_prev, Inf)
        ## Primal infeasibility
        inf_pr = ExaPF.primal_infeasibility(nlp.inner, nlp.cons)

        # check convergence
        if (i % verbose_it == 0)
            @printf("%6d %.6e %.3e %.2e %.2e %.2e\n", i, c, c - c_ref, norm_grad, inf_pr, step)
        end

        if bfgs
            push!(H, uk .- u_prev, grad .- grad_prev)
            grad_prev .= grad
        end
        u_prev .= uk
        # Check whether we have converged nicely
        if (norm_grad < tol)
            converged = true
            break
        end
    end
    return uk, norm_grad, n_iter
end


# Non-monotone gradient projection algorithm
function ngpa(
    nlp,
    uk;
    active_set=false,
    max_iter=1000,
    ls_itermax=30,
    Œ±_bb=1.0,
    Œ±‚ô≠=0.0,
    Œ±‚ôØ=1.0,
    Œ≤=0.4,
    Œ¥=1e-4,
    tol=1e-4,
    verbose_it=Inf,
    ls_algo=2,
)

    u_prev = copy(uk)
    grad = copy(uk)
    wk = copy(uk)
    u‚ô≠ = nlp.inner.u_min
    u‚ôØ = nlp.inner.u_max

    # Initial evaluation
    ExaPF.update!(nlp, uk)
    f = ExaPF.objective(nlp, uk)
    ExaPF.gradient!(nlp, grad, uk)
    # Memory
    grad_prev = copy(grad)
    # Active gradient
    grad_act = copy(grad)

    norm_grad = norm(grad, Inf)
    n_iter = 0
    ## Line-search params
    j_bb = 0
    flag_bb = 1
    Œ∏_bb = 0.975
    m_bb = 10
    ## Reference function params
    L_ref = 3
    M_ref = 8
    P_ref = 40
    Œ≥1_ref = M_ref / L_ref
    Œ≥2_ref = P_ref / M_ref
    l_ref = 0
    p_ref = 0
    f‚ô≠_ref = f
    f‚ôØ_ref = f
    fc_ref = f
    buffer_costs = Float64[f for i in 1:M_ref]

    Œ∑_ref = 0.8
    Q_ref = 1.0
    f·µ£ = f
    # Armijo params
    œÉ1_arm = 0.1
    œÉ2_arm = 0.9

    # Active set
    ùîò  = Int[]
    ùîÑ  = Int[]
    ùîÑ_hash_1 = hash(ùîÑ)
    ùîÑ_hash_2 = hash(ùîÑ)
    Œº_act = 0.1
    œÅ_act = 0.5

    n_up = 0
    for i in 1:max_iter
        n_iter += 1

        ExaPF.project!(wk, uk .- Œ±_bb .* grad, u‚ô≠, u‚ôØ)
        # Feasible direction
        dk = wk .- uk

        ##################################################
        # Armijo line-search
        step = 1.0
        d‚àág = dot(dk, grad)
        for j_ls in 1:ls_itermax
            ExaPF.project!(wk, uk .+ step .* dk, u‚ô≠, u‚ôØ)
            conv = ExaPF.update!(nlp, wk)
            ft = ExaPF.objective(nlp, wk)
            if ft <= min(f·µ£, f‚ôØ_ref) + step * Œ¥ * d‚àág
                break
            end
            step *= Œ≤
            # Step introduced in Birgin & Martinez & Raydan (2012)
            Œ± = - 0.5 * step^2 * d‚àág / (ft - f - step * d‚àág)
            if œÉ1_arm * step <= Œ± <= œÉ2_arm * step
                step = Œ±
            else
                step *= Œ≤
            end
        end

        uk .= wk
        # Objective
        f = ExaPF.objective(nlp, uk)
        c_ref = ExaPF.inner_objective(nlp, uk)
        # Gradient
        ExaPF.gradient!(nlp, grad, uk)

        # Stopping criteration: u‚Çñ‚Çä‚ÇÅ - u‚Çñ
        ## Dual infeasibility
        norm_grad = norm(dk, Inf)
        ## Primal infeasibility
        inf_pr = ExaPF.primal_infeasibility(nlp.inner, nlp.cons ./ nlp.scaler.scale_cons)

        # check convergence
        if (i % verbose_it == 0)
            @printf("%6d %.6e %.3e %.2e %.2e %.2e\n", i, f, f - c_ref, norm_grad, inf_pr, step)
        end

        ##################################################
        ## Update parameters
        sk = uk - u_prev
        yk = grad - grad_prev

        ##################################################
        ## Update Barzilai-Borwein step
        flag_bb = 0
        if !isnothing(findfirst(0.0 .< abs.(dk) .< Œ±_bb .* abs.(grad)))
            flag_bb = 1
        end
        if step == 1.0
            j_bb += 1
        else
            flag_bb = 1
        end
        Œ∏ = dot(sk, yk) / (norm(sk) * norm(yk))
        if (j_bb >= m_bb) || (Œ∏ >= Œ∏_bb) || (flag_bb == 1)
            # Non-convexity detected
            if dot(sk, yk) <= 0.0
                if j_bb >= 1.5 * m_bb
                    t_bb = min(norm(uk, Inf), 1) / norm(dk, Inf)
                    Œ±_bb = max(t, step)
                    j_bb = 0
                end
            else
                # Everything went ok. Set new step to Barzilai-Borwein value
                Œ±_bb = dot(sk, sk) / dot(sk, yk)
                j_bb = 0
            end
        end
        Œ±_bb = min(Œ±‚ôØ, Œ±_bb)
        # Update history
        u_prev .= uk
        grad_prev .= grad

        ##################################################
        ## Update reference value
        # Update maximum value
        buffer_costs[i % M_ref + 1] = f
        f‚ôØ_ref = maximum(buffer_costs)
        if ls_algo == 1
            w = .0
            f·µ£ = w * f‚ôØ_ref + (1 - w) * f
        elseif ls_algo == 2
            qt = Œ∑_ref * Q_ref + 1.0
            f·µ£ = (Œ∑_ref * Q_ref * f·µ£ + f) / qt
            Q_ref = qt
        elseif ls_algo == 3
            # Update parameters
            p_ref = (step == 1.0) ? p_ref + 1 : 0
            if f < f‚ô≠_ref
                fc_ref = f
                f‚ô≠_ref = f
                l_ref = 0
            else
                l_ref += 1
            end
            fc_ref = (f > fc_ref) ? f : fc_ref

            # Step 1
            if l_ref == L_ref
                ratio = (f‚ôØ_ref - f‚ô≠_ref) / (fc_ref - f‚ô≠_ref)
                f·µ£ = (ratio > Œ≥1_ref) ? fc_ref : f‚ôØ_ref
            end
            if p_ref > P_ref
                ratio = (f·µ£ - f) / (f‚ôØ_ref - f)
                f·µ£ = (f‚ôØ_ref > f) && (ratio >= Œ≥2_ref) ? f‚ôØ_ref : f·µ£
            end
        end

        # Active-set embedding
        if active_set && (i >= 10)
            grad_act .= grad
            # Compute U
            empty!(ùîò)
            empty!(ùîÑ)
            ExaOpt.active!(grad_act, uk, u‚ô≠, u‚ôØ)
            ndk = norm(dk, Inf)
            # Update active set
            for i in eachindex(uk)
                if abs(grad[i] >= sqrt(ndk)) && (uk[i] >= ndk^1.5)
                    push!(ùîò, i)
                end
                if (uk[i] > u‚ô≠[i]) || (uk[i] < u‚ôØ[i])
                    push!(ùîÑ, i)
                end
            end
            if ùîÑ_hash_1 == ùîÑ_hash_2 == hash(ùîÑ)
                if norm(grad_act, Inf) >= Œº_act * ndk
                    break
                end
            end
            if isempty(ùîò)
                if norm(grad_act, Inf) < Œº_act * ndk
                    Œº_act = œÅ_act * Œº_act
                else
                    break
                end
            end
            ùîÑ_hash_1 = ùîÑ_hash_2
            ùîÑ_hash_2 = hash(ùîÑ)
        end

        # Check whether we have converged nicely
        if (norm_grad < tol)
            converged = true
            break
        end
    end
    return uk, norm_grad, n_iter
end

